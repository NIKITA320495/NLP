{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc56f19",
   "metadata": {
    "papermill": {
     "duration": 0.005344,
     "end_time": "2025-01-26T14:08:35.862087",
     "exception": false,
     "start_time": "2025-01-26T14:08:35.856743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Word2Vec from Scratch\n",
    "\n",
    "In this notebook, we will implement **Word2Vec** from scratch, a popular technique for learning distributed word representations. Word2Vec converts words into dense vector embeddings that capture semantic relationships based on the surrounding context in a text corpus. We will focus on both **CBOW** and **Skip-Gram** architecture.\n",
    "\n",
    "We will walk through the entire process:\n",
    "- **Preprocessing the corpus** to tokenize, clean, and build a vocabulary.\n",
    "- **Generating training data** for the model by creating target-context pairs.\n",
    "- **Initializing and training the model** by updating weights using the CBOW and  Skip-Gram approach.\n",
    "- **Extracting word embeddings** to visualize the learned word vectors.\n",
    "\n",
    "## What is Word2Vec?\n",
    "\n",
    "Word2Vec is a machine learning technique that transforms words into dense vector representations, capturing semantic relationships based on their context in a corpus. It uses two architectures: **Skip-Gram**, which predicts context words given a target word, and **CBOW (Continuous Bag of Words)**, which predicts a target word from surrounding context words. By training on large text data, Word2Vec learns embeddings where words with similar meanings are positioned closer in the vector space, enabling efficient natural language processing tasks like sentiment analysis, clustering, and recommendation systems.\n",
    "\n",
    "### CBOW (Continuous Bag of Words)\n",
    "The **Continuous Bag of Words (CBOW)** model is one of the two architectures in Word2Vec. It aims to predict a target word based on the surrounding context words. Here's how it works:\n",
    "1. **Context Window**: For a given target word, a context window is selected (typically a fixed number of words before and after the target word).\n",
    "2. **Prediction**: CBOW uses the context words (surrounding words) to predict the target word (center word). The model tries to maximize the probability of the target word, given the context.\n",
    "   \n",
    "For example, in the sentence **\"The cat sat on the mat,\"** if the target word is \"sat,\" the context words would be **[\"the\", \"cat\", \"on\", \"the\", \"mat\"]**. The model uses these context words to predict the word \"sat.\" \n",
    "\n",
    "This approach works well for smaller datasets and is efficient when the task involves predicting target words based on context.\n",
    "\n",
    "### Skip-Gram Model\n",
    "The **Skip-Gram** model is the second architecture in Word2Vec. Unlike CBOW, Skip-Gram tries to predict the context words based on a given target word. Instead of predicting a single target word from context, Skip-Gram focuses on maximizing the probability of context words given the target word. \n",
    "\n",
    "For example, using the same sentence **\"The cat sat on the mat,\"** if \"sat\" is the target word, the model tries to predict context words **[\"the\", \"cat\", \"on\", \"the\", \"mat\"]**. \n",
    "\n",
    "Skip-Gram is more effective in cases where we have large corpora and is particularly good at handling rare words by using context.\n",
    "\n",
    "### Key Differences Between CBOW and Skip-Gram\n",
    "- **CBOW** uses context words to predict the target word, making it faster for training and better for smaller datasets.\n",
    "- **Skip-Gram** predicts the context words based on the target word, which is more effective for large datasets and rare words.\n",
    "- **Training Efficiency**: CBOW is faster due to fewer weight updates as it predicts one word from the context, while Skip-Gram requires updating weights for multiple context words per target word.\n",
    "\n",
    "Both architectures aim to generate vector embeddings where words with similar meanings are placed closer in the vector space, and can be applied to a wide range of tasks like sentiment analysis, machine translation, and word similarity analysis.\n",
    "\n",
    "---\n",
    "\n",
    "In the upcoming sections of this notebook, we will implement both CBOW and Skip-Gram models from scratch, starting with preprocessing the dataset and building the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce14f6e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-26T14:08:35.873067Z",
     "iopub.status.busy": "2025-01-26T14:08:35.872709Z",
     "iopub.status.idle": "2025-01-26T14:08:36.764071Z",
     "shell.execute_reply": "2025-01-26T14:08:36.762970Z"
    },
    "papermill": {
     "duration": 0.89892,
     "end_time": "2025-01-26T14:08:36.765974",
     "exception": false,
     "start_time": "2025-01-26T14:08:35.867054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f24c69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T14:08:36.777032Z",
     "iopub.status.busy": "2025-01-26T14:08:36.776525Z",
     "iopub.status.idle": "2025-01-26T14:08:38.789098Z",
     "shell.execute_reply": "2025-01-26T14:08:38.787868Z"
    },
    "papermill": {
     "duration": 2.020225,
     "end_time": "2025-01-26T14:08:38.791163",
     "exception": false,
     "start_time": "2025-01-26T14:08:36.770938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77d00ddc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T14:08:38.802221Z",
     "iopub.status.busy": "2025-01-26T14:08:38.801774Z",
     "iopub.status.idle": "2025-01-26T14:08:38.805972Z",
     "shell.execute_reply": "2025-01-26T14:08:38.805102Z"
    },
    "papermill": {
     "duration": 0.011427,
     "end_time": "2025-01-26T14:08:38.807504",
     "exception": false,
     "start_time": "2025-01-26T14:08:38.796077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The sky is blue and beautiful\",\n",
    "    \"Love this blue and beautiful sky\",\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"A king's breakfast has sausages, ham, bacon, eggs, toast, and beans\",\n",
    "    \"I love green eggs, ham, sausages, and bacon\",\n",
    "    \"The brown fox is quick and the blue dog is lazy\",\n",
    "    \"The sky is very blue and the sky is very beautiful today\",\n",
    "    \"The dog is lazy but the brown fox is quick\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bb66356",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T14:08:38.818298Z",
     "iopub.status.busy": "2025-01-26T14:08:38.817972Z",
     "iopub.status.idle": "2025-01-26T14:08:38.823100Z",
     "shell.execute_reply": "2025-01-26T14:08:38.822204Z"
    },
    "papermill": {
     "duration": 0.012242,
     "end_time": "2025-01-26T14:08:38.824675",
     "exception": false,
     "start_time": "2025-01-26T14:08:38.812433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()\n",
    "def text_cleaning(corpus):\n",
    "    cleaned_corpus=[]\n",
    "    for review in corpus:\n",
    "        review=re.sub('[^a-zA-Z]',' ',review)\n",
    "        review=review.lower().split()\n",
    "        review=[stemmer.stem(word) for word in review if word not in stopwords.words('english')]\n",
    "        review=' '.join (review)\n",
    "        cleaned_corpus.append(review)\n",
    "    return cleaned_corpus  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f481784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T14:08:38.835589Z",
     "iopub.status.busy": "2025-01-26T14:08:38.835221Z",
     "iopub.status.idle": "2025-01-26T14:08:38.856602Z",
     "shell.execute_reply": "2025-01-26T14:08:38.855691Z"
    },
    "papermill": {
     "duration": 0.028499,
     "end_time": "2025-01-26T14:08:38.858150",
     "exception": false,
     "start_time": "2025-01-26T14:08:38.829651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sky blue beauti',\n",
       " 'love blue beauti sky',\n",
       " 'quick brown fox jump lazi dog',\n",
       " 'king breakfast sausag ham bacon egg toast bean',\n",
       " 'love green egg ham sausag bacon',\n",
       " 'brown fox quick blue dog lazi',\n",
       " 'sky blue sky beauti today',\n",
       " 'dog lazi brown fox quick']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=text_cleaning(corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c77f249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T14:08:38.869012Z",
     "iopub.status.busy": "2025-01-26T14:08:38.868686Z",
     "iopub.status.idle": "2025-01-26T14:08:38.874849Z",
     "shell.execute_reply": "2025-01-26T14:08:38.873962Z"
    },
    "papermill": {
     "duration": 0.013305,
     "end_time": "2025-01-26T14:08:38.876338",
     "exception": false,
     "start_time": "2025-01-26T14:08:38.863033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sky', 'blue', 'beauti'],\n",
       " ['love', 'blue', 'beauti', 'sky'],\n",
       " ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog'],\n",
       " ['king', 'breakfast', 'sausag', 'ham', 'bacon', 'egg', 'toast', 'bean'],\n",
       " ['love', 'green', 'egg', 'ham', 'sausag', 'bacon'],\n",
       " ['brown', 'fox', 'quick', 'blue', 'dog', 'lazi'],\n",
       " ['sky', 'blue', 'sky', 'beauti', 'today'],\n",
       " ['dog', 'lazi', 'brown', 'fox', 'quick']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus = [sentence.split() for sentence in corpus]\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f94cf54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T14:08:38.887536Z",
     "iopub.status.busy": "2025-01-26T14:08:38.887200Z",
     "iopub.status.idle": "2025-01-26T14:08:38.897943Z",
     "shell.execute_reply": "2025-01-26T14:08:38.896783Z"
    },
    "papermill": {
     "duration": 0.018121,
     "end_time": "2025-01-26T14:08:38.899561",
     "exception": false,
     "start_time": "2025-01-26T14:08:38.881440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sky\n",
      "blue\n",
      "beauti\n",
      "love\n",
      "blue\n",
      "beauti\n",
      "sky\n",
      "quick\n",
      "brown\n",
      "fox\n",
      "jump\n",
      "lazi\n",
      "dog\n",
      "king\n",
      "breakfast\n",
      "sausag\n",
      "ham\n",
      "bacon\n",
      "egg\n",
      "toast\n",
      "bean\n",
      "love\n",
      "green\n",
      "egg\n",
      "ham\n",
      "sausag\n",
      "bacon\n",
      "brown\n",
      "fox\n",
      "quick\n",
      "blue\n",
      "dog\n",
      "lazi\n",
      "sky\n",
      "blue\n",
      "sky\n",
      "beauti\n",
      "today\n",
      "dog\n",
      "lazi\n",
      "brown\n",
      "fox\n",
      "quick\n"
     ]
    }
   ],
   "source": [
    "for sentence in tokenized_corpus:\n",
    "    for word in sentence:\n",
    "        print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cc5f956",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T14:08:38.911917Z",
     "iopub.status.busy": "2025-01-26T14:08:38.911539Z",
     "iopub.status.idle": "2025-01-26T14:08:38.916327Z",
     "shell.execute_reply": "2025-01-26T14:08:38.915341Z"
    },
    "papermill": {
     "duration": 0.012364,
     "end_time": "2025-01-26T14:08:38.918071",
     "exception": false,
     "start_time": "2025-01-26T14:08:38.905707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_vocab(corpus):\n",
    "    # Flatten the corpus into one list and count the frequency of words\n",
    "    words = [word for sentence in corpus for word in sentence]\n",
    "    word_counts = Counter(words)\n",
    "    vocabulary = {word: idx for idx, (word, _) in enumerate(word_counts.items())}\n",
    "    return vocabulary, word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afcbdf3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T14:08:38.929443Z",
     "iopub.status.busy": "2025-01-26T14:08:38.929108Z",
     "iopub.status.idle": "2025-01-26T14:08:38.933237Z",
     "shell.execute_reply": "2025-01-26T14:08:38.932376Z"
    },
    "papermill": {
     "duration": 0.011641,
     "end_time": "2025-01-26T14:08:38.934856",
     "exception": false,
     "start_time": "2025-01-26T14:08:38.923215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocabulary, word_counts = build_vocab(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25940f75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T14:08:38.946477Z",
     "iopub.status.busy": "2025-01-26T14:08:38.946088Z",
     "iopub.status.idle": "2025-01-26T14:08:38.952005Z",
     "shell.execute_reply": "2025-01-26T14:08:38.951159Z"
    },
    "papermill": {
     "duration": 0.013556,
     "end_time": "2025-01-26T14:08:38.953621",
     "exception": false,
     "start_time": "2025-01-26T14:08:38.940065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'sky': 4,\n",
       "         'blue': 4,\n",
       "         'beauti': 3,\n",
       "         'love': 2,\n",
       "         'quick': 3,\n",
       "         'brown': 3,\n",
       "         'fox': 3,\n",
       "         'jump': 1,\n",
       "         'lazi': 3,\n",
       "         'dog': 3,\n",
       "         'king': 1,\n",
       "         'breakfast': 1,\n",
       "         'sausag': 2,\n",
       "         'ham': 2,\n",
       "         'bacon': 2,\n",
       "         'egg': 2,\n",
       "         'toast': 1,\n",
       "         'bean': 1,\n",
       "         'green': 1,\n",
       "         'today': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dec76bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T14:08:38.965232Z",
     "iopub.status.busy": "2025-01-26T14:08:38.964916Z",
     "iopub.status.idle": "2025-01-26T14:08:38.970500Z",
     "shell.execute_reply": "2025-01-26T14:08:38.969649Z"
    },
    "papermill": {
     "duration": 0.013117,
     "end_time": "2025-01-26T14:08:38.972073",
     "exception": false,
     "start_time": "2025-01-26T14:08:38.958956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sky': 0,\n",
       " 'blue': 1,\n",
       " 'beauti': 2,\n",
       " 'love': 3,\n",
       " 'quick': 4,\n",
       " 'brown': 5,\n",
       " 'fox': 6,\n",
       " 'jump': 7,\n",
       " 'lazi': 8,\n",
       " 'dog': 9,\n",
       " 'king': 10,\n",
       " 'breakfast': 11,\n",
       " 'sausag': 12,\n",
       " 'ham': 13,\n",
       " 'bacon': 14,\n",
       " 'egg': 15,\n",
       " 'toast': 16,\n",
       " 'bean': 17,\n",
       " 'green': 18,\n",
       " 'today': 19}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b7300ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T14:08:38.984144Z",
     "iopub.status.busy": "2025-01-26T14:08:38.983806Z",
     "iopub.status.idle": "2025-01-26T14:08:38.989026Z",
     "shell.execute_reply": "2025-01-26T14:08:38.988048Z"
    },
    "papermill": {
     "duration": 0.012822,
     "end_time": "2025-01-26T14:08:38.990537",
     "exception": false,
     "start_time": "2025-01-26T14:08:38.977715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size=len(vocabulary)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0bd1ea9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T14:08:39.002628Z",
     "iopub.status.busy": "2025-01-26T14:08:39.002241Z",
     "iopub.status.idle": "2025-01-26T14:08:39.006251Z",
     "shell.execute_reply": "2025-01-26T14:08:39.005368Z"
    },
    "papermill": {
     "duration": 0.012064,
     "end_time": "2025-01-26T14:08:39.008006",
     "exception": false,
     "start_time": "2025-01-26T14:08:38.995942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "context_window = 2  # Number of words around the target word\n",
    "embedding_dim = 5  # Dimensionality of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25208db2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T14:08:39.020203Z",
     "iopub.status.busy": "2025-01-26T14:08:39.019886Z",
     "iopub.status.idle": "2025-01-26T14:08:39.038582Z",
     "shell.execute_reply": "2025-01-26T14:08:39.037272Z"
    },
    "papermill": {
     "duration": 0.027291,
     "end_time": "2025-01-26T14:08:39.040789",
     "exception": false,
     "start_time": "2025-01-26T14:08:39.013498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder=OneHotEncoder(sparse=False)\n",
    "encoded_vocab=encoder.fit_transform(np.array(list(vocabulary.values())).reshape(-1,1))\n",
    "encoded_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39b00753",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T14:08:39.053377Z",
     "iopub.status.busy": "2025-01-26T14:08:39.053051Z",
     "iopub.status.idle": "2025-01-26T14:08:39.069802Z",
     "shell.execute_reply": "2025-01-26T14:08:39.068431Z"
    },
    "papermill": {
     "duration": 0.025089,
     "end_time": "2025-01-26T14:08:39.071565",
     "exception": false,
     "start_time": "2025-01-26T14:08:39.046476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sky\n",
      "1 blue\n",
      "2 beauti\n",
      "0 love\n",
      "1 blue\n",
      "2 beauti\n",
      "3 sky\n",
      "0 quick\n",
      "1 brown\n",
      "2 fox\n",
      "3 jump\n",
      "4 lazi\n",
      "5 dog\n",
      "0 king\n",
      "1 breakfast\n",
      "2 sausag\n",
      "3 ham\n",
      "4 bacon\n",
      "5 egg\n",
      "6 toast\n",
      "7 bean\n",
      "0 love\n",
      "1 green\n",
      "2 egg\n",
      "3 ham\n",
      "4 sausag\n",
      "5 bacon\n",
      "0 brown\n",
      "1 fox\n",
      "2 quick\n",
      "3 blue\n",
      "4 dog\n",
      "5 lazi\n",
      "0 sky\n",
      "1 blue\n",
      "2 sky\n",
      "3 beauti\n",
      "4 today\n",
      "0 dog\n",
      "1 lazi\n",
      "2 brown\n",
      "3 fox\n",
      "4 quick\n"
     ]
    }
   ],
   "source": [
    " for sentence in tokenized_corpus:\n",
    "        for idx,target_word in enumerate(sentence):\n",
    "            print(idx,target_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad895d98",
   "metadata": {
    "papermill": {
     "duration": 0.005429,
     "end_time": "2025-01-26T14:08:39.082841",
     "exception": false,
     "start_time": "2025-01-26T14:08:39.077412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generating CBOW data\n",
    "**Input**: It takes a corpus (list of tokenized sentences) and a context_window (an integer that defines how many words before and after the target word will be considered as context).\n",
    "\n",
    "**Context-Target Pairing**:\n",
    "\n",
    "For each word in the sentence (target_word), it selects a context window around it.\n",
    "The context window is defined by the context_window size, which gives the surrounding words (not including the target word itself).\n",
    "\n",
    "**Building Context and Target:**\n",
    "\n",
    "Context: All words in the sentence that fall within the context window around the target word.\n",
    "\n",
    "Target: The target word itself, converted to its index in the vocabulary.\n",
    "\n",
    "**Output:** The function appends pairs of context (list of word indices) and target (single word index) to the context_target_pairs list. This list contains all the (context, target) pairs needed for training the CBOW model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1584ddd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T14:08:39.095621Z",
     "iopub.status.busy": "2025-01-26T14:08:39.095225Z",
     "iopub.status.idle": "2025-01-26T14:08:39.106679Z",
     "shell.execute_reply": "2025-01-26T14:08:39.105630Z"
    },
    "papermill": {
     "duration": 0.019661,
     "end_time": "2025-01-26T14:08:39.108431",
     "exception": false,
     "start_time": "2025-01-26T14:08:39.088770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([1, 2], 0),\n",
       " ([0, 2], 1),\n",
       " ([0, 1], 2),\n",
       " ([1, 2], 3),\n",
       " ([3, 2, 0], 1),\n",
       " ([3, 1, 0], 2),\n",
       " ([1, 2], 0),\n",
       " ([5, 6], 4),\n",
       " ([4, 6, 7], 5),\n",
       " ([4, 5, 7, 8], 6),\n",
       " ([5, 6, 8, 9], 7),\n",
       " ([6, 7, 9], 8),\n",
       " ([7, 8], 9),\n",
       " ([11, 12], 10),\n",
       " ([10, 12, 13], 11),\n",
       " ([10, 11, 13, 14], 12),\n",
       " ([11, 12, 14, 15], 13),\n",
       " ([12, 13, 15, 16], 14),\n",
       " ([13, 14, 16, 17], 15),\n",
       " ([14, 15, 17], 16),\n",
       " ([15, 16], 17),\n",
       " ([18, 15], 3),\n",
       " ([3, 15, 13], 18),\n",
       " ([3, 18, 13, 12], 15),\n",
       " ([18, 15, 12, 14], 13),\n",
       " ([15, 13, 14], 12),\n",
       " ([13, 12], 14),\n",
       " ([6, 4], 5),\n",
       " ([5, 4, 1], 6),\n",
       " ([5, 6, 1, 9], 4),\n",
       " ([6, 4, 9, 8], 1),\n",
       " ([4, 1, 8], 9),\n",
       " ([1, 9], 8),\n",
       " ([1, 0], 0),\n",
       " ([0, 0, 2], 1),\n",
       " ([0, 1, 2, 19], 0),\n",
       " ([1, 0, 19], 2),\n",
       " ([0, 2], 19),\n",
       " ([8, 5], 9),\n",
       " ([9, 5, 6], 8),\n",
       " ([9, 8, 6, 4], 5),\n",
       " ([8, 5, 4], 6),\n",
       " ([5, 6], 4)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_cbow_data(corpus,contect_window):\n",
    "    context_target_pairs=[]\n",
    "    for sentence in corpus:\n",
    "        for idx,target_word in enumerate(sentence):\n",
    "            start=max(0, idx-context_window)\n",
    "            end=min(len(sentence), idx+context_window+1)\n",
    "\n",
    "            context=[sentence[i] for i in range(start, end) if i !=idx]\n",
    "            target_idx=vocabulary[target_word]\n",
    "            context_idx=[vocabulary[word] for word in context]\n",
    "            context_target_pairs.append((context_idx,target_idx))\n",
    "    return context_target_pairs\n",
    "context_target_pairs = generate_cbow_data(tokenized_corpus, context_window)\n",
    "context_target_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fdae100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T14:08:39.121538Z",
     "iopub.status.busy": "2025-01-26T14:08:39.121124Z",
     "iopub.status.idle": "2025-01-26T14:08:39.133057Z",
     "shell.execute_reply": "2025-01-26T14:08:39.132052Z"
    },
    "papermill": {
     "duration": 0.020179,
     "end_time": "2025-01-26T14:08:39.134676",
     "exception": false,
     "start_time": "2025-01-26T14:08:39.114497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['blue', 'beauti'], 'sky'),\n",
       " (['sky', 'beauti'], 'blue'),\n",
       " (['sky', 'blue'], 'beauti'),\n",
       " (['blue', 'beauti'], 'love'),\n",
       " (['love', 'beauti', 'sky'], 'blue'),\n",
       " (['love', 'blue', 'sky'], 'beauti'),\n",
       " (['blue', 'beauti'], 'sky'),\n",
       " (['brown', 'fox'], 'quick'),\n",
       " (['quick', 'fox', 'jump'], 'brown'),\n",
       " (['quick', 'brown', 'jump', 'lazi'], 'fox'),\n",
       " (['brown', 'fox', 'lazi', 'dog'], 'jump'),\n",
       " (['fox', 'jump', 'dog'], 'lazi'),\n",
       " (['jump', 'lazi'], 'dog'),\n",
       " (['breakfast', 'sausag'], 'king'),\n",
       " (['king', 'sausag', 'ham'], 'breakfast'),\n",
       " (['king', 'breakfast', 'ham', 'bacon'], 'sausag'),\n",
       " (['breakfast', 'sausag', 'bacon', 'egg'], 'ham'),\n",
       " (['sausag', 'ham', 'egg', 'toast'], 'bacon'),\n",
       " (['ham', 'bacon', 'toast', 'bean'], 'egg'),\n",
       " (['bacon', 'egg', 'bean'], 'toast'),\n",
       " (['egg', 'toast'], 'bean'),\n",
       " (['green', 'egg'], 'love'),\n",
       " (['love', 'egg', 'ham'], 'green'),\n",
       " (['love', 'green', 'ham', 'sausag'], 'egg'),\n",
       " (['green', 'egg', 'sausag', 'bacon'], 'ham'),\n",
       " (['egg', 'ham', 'bacon'], 'sausag'),\n",
       " (['ham', 'sausag'], 'bacon'),\n",
       " (['fox', 'quick'], 'brown'),\n",
       " (['brown', 'quick', 'blue'], 'fox'),\n",
       " (['brown', 'fox', 'blue', 'dog'], 'quick'),\n",
       " (['fox', 'quick', 'dog', 'lazi'], 'blue'),\n",
       " (['quick', 'blue', 'lazi'], 'dog'),\n",
       " (['blue', 'dog'], 'lazi'),\n",
       " (['blue', 'sky'], 'sky'),\n",
       " (['sky', 'sky', 'beauti'], 'blue'),\n",
       " (['sky', 'blue', 'beauti', 'today'], 'sky'),\n",
       " (['blue', 'sky', 'today'], 'beauti'),\n",
       " (['sky', 'beauti'], 'today'),\n",
       " (['lazi', 'brown'], 'dog'),\n",
       " (['dog', 'brown', 'fox'], 'lazi'),\n",
       " (['dog', 'lazi', 'fox', 'quick'], 'brown'),\n",
       " (['lazi', 'brown', 'quick'], 'fox'),\n",
       " (['brown', 'fox'], 'quick')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_cbow_data_inwords(corpus, context_window):\n",
    "    context_target_pairs = []\n",
    "    # Reverse the vocabulary dictionary to map index to word\n",
    "    index_to_word = {index: word for word, index in vocabulary.items()}\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        for idx, target_word in enumerate(sentence):\n",
    "            start = max(0, idx - context_window)\n",
    "            end = min(len(sentence), idx + context_window + 1)\n",
    "\n",
    "            context = [sentence[i] for i in range(start, end) if i != idx]\n",
    "            \n",
    "            # Get the words instead of indices\n",
    "            target_word_in_vocab = target_word  # Target word remains the same\n",
    "            context_words = [sentence[i] for i in range(start, end) if i != idx]\n",
    "\n",
    "            # Append the word-context pairs\n",
    "            context_target_pairs.append((context_words, target_word_in_vocab))\n",
    "\n",
    "    return context_target_pairs\n",
    "context_target_pairs_inwords = generate_cbow_data_inwords(tokenized_corpus, context_window)\n",
    "context_target_pairs_inwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "576384d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T14:08:39.147763Z",
     "iopub.status.busy": "2025-01-26T14:08:39.147388Z",
     "iopub.status.idle": "2025-01-26T14:08:39.154103Z",
     "shell.execute_reply": "2025-01-26T14:08:39.153142Z"
    },
    "papermill": {
     "duration": 0.014853,
     "end_time": "2025-01-26T14:08:39.155644",
     "exception": false,
     "start_time": "2025-01-26T14:08:39.140791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_weights(vocab_size,embedding_dim):\n",
    "    W_in = np.random.uniform(-1,1,(vocab_size,embedding_dim))  # Initialize input weights (context to hidden layer)\n",
    "    # Initialize output weights (hidden to output layer)\n",
    "    W_out = np.random.uniform(-1, 1, (embedding_dim, vocab_size))\n",
    "    return W_in, W_out\n",
    "W_in, W_out = initialize_weights(vocab_size, embedding_dim)\n",
    "len(W_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "037043cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T14:08:39.169521Z",
     "iopub.status.busy": "2025-01-26T14:08:39.169192Z",
     "iopub.status.idle": "2025-01-26T14:08:39.173241Z",
     "shell.execute_reply": "2025-01-26T14:08:39.172327Z"
    },
    "papermill": {
     "duration": 0.01285,
     "end_time": "2025-01-26T14:08:39.174845",
     "exception": false,
     "start_time": "2025-01-26T14:08:39.161995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def train_cbow(corpus, context_target_pairs,W_in,W_out,epochs=10,learning_rate=0.01):\n",
    "#     for epoch in range(epochs):\n",
    "#         total_loss=0\n",
    "#         for context_words,target_word in context_target_pairs:\n",
    "#             context_vectors=np.mean(W_in[context_words],axis=0)\n",
    "#             predicted_word_vector=np.dot(context_vectors,W_out)\n",
    "#             # Compute the softmax function for output layer\n",
    "#             exp_scores=np.exp(predicted_word_vector)\n",
    "#             probs=exp_scores/np.sum(exp_scores)\n",
    "#             # Compute loss\n",
    "#             target_vector=np.zeros(vocab_size)\n",
    "#             target_vector[target_word]=1\n",
    "#             loss= -np.log(probs[target_word])\n",
    "#             total_loss +=loss\n",
    "#             # Backpropogation\n",
    "#             error=probs -target_vector\n",
    "#             dW_out= np.outer(context_vectors,error)\n",
    "#             dW_in=np.outer(error,W_out.T).mean(axis=0)\n",
    "#             # Update Weights\n",
    "#             W_in[context_words]-= learning_rate * dW_in\n",
    "#             W_out -= learning_rate* dW_out\n",
    "#     print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}\")\n",
    "\n",
    "# # Train the CBOW model\n",
    "# train_cbow(tokenized_corpus, context_target_pairs, W_in, W_out, epochs=100, learning_rate=0.01)\n",
    "\n",
    "# # Extract learned word embeddings\n",
    "# word_embeddings = W_in\n",
    "# print(\"Learned word embeddings:\")\n",
    "# print(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f6c746",
   "metadata": {
    "papermill": {
     "duration": 0.006748,
     "end_time": "2025-01-26T14:08:39.187963",
     "exception": false,
     "start_time": "2025-01-26T14:08:39.181215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.741778,
   "end_time": "2025-01-26T14:08:39.913451",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-26T14:08:33.171673",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
